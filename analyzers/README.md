# RecommendationAnalyzer Documentation

## Overview

The `RecommendationAnalyzer` is the intelligent recommendation engine of the work order optimization system, responsible for analyzing historical patterns, alert triggers, and performance metrics to generate proactive task recommendations. Following clean architecture principles, it focuses on "What should we do next?" while providing data-driven insights for optimal facility maintenance.

## Architecture Philosophy

**Single Responsibility**: Pure recommendation analysis and prediction
**Input**: Historical work orders, alerts, metrics, employee availability
**Output**: Intelligent task recommendations with confidence scoring
**No Scheduling**: Raw recommendations only - scheduling handled by separate components

## Core Recommendation System

The RecommendationAnalyzer provides intelligent, data-driven recommendations based on multiple analysis vectors, with all methods designed for scalability and accuracy through machine learning-style pattern recognition.

---

## Implementation Details

### Initialization and Data Setup

```python
def __init__(self, database_connection):
    self.db = database_connection
    self.employees = self._load_active_employees()
    self.locations = self._load_locations()
    self.templates = self._load_task_templates()
```

The analyzer initializes with:
- **Database Integration**: Full database connectivity for historical analysis
- **Active Employee Pool**: Current workforce with skills and availability
- **Location Intelligence**: Complete facility mapping with priority scoring
- **Task Templates**: Standardized task definitions for consistency

---

## Core Analysis Methods

### 1. Historical Pattern Analysis

#### `analyze_historical_patterns(location_id: str, work_order_type: str, lookback_days: int = 30) -> Dict[str, Any]`

**Purpose**: Analyzes historical completion patterns to predict optimal timing and resources for future work orders

#### Implementation Details:
- **Pattern Recognition**: Advanced statistical analysis of completion intervals
- **Quality Correlation**: Links historical quality scores to timing patterns
- **Assignee Performance**: Tracks most effective employee assignments
- **Confidence Scoring**: ML-style confidence calculation based on data quality

#### Key Algorithms:

##### Completion Interval Analysis:
```python
# Statistical pattern recognition
completion_intervals = []
for i in range(1, len(historical_orders)):
    interval = (historical_orders[i]['actual_end_time'] -
               historical_orders[i-1]['actual_end_time']).days
    completion_intervals.append(interval)

# Pattern strength calculation
avg_interval = sum(completion_intervals) / len(completion_intervals)
interval_variance = sum((x - avg_interval) ** 2 for x in completion_intervals) / len(completion_intervals)
pattern_strength = 1.0 / (1.0 + interval_variance / avg_interval)
```

##### Predictive Next Occurrence:
```python
# Predict next recommended date
last_completion = max(order['actual_end_time'] for order in historical_orders)
predicted_next = last_completion + timedelta(days=avg_interval)
```

##### Confidence Scoring Algorithm:
```python
# ML-style confidence calculation
recommendation_confidence = min(0.95, pattern_strength * 0.8 + len(historical_orders) * 0.05)
```

#### Analysis Components:
- **Data Sufficiency**: Minimum 3 historical orders for reliable patterns
- **Quality Metrics**: Average quality and efficiency score tracking
- **Assignee Analytics**: Performance-based assignee recommendations
- **Variance Analysis**: Pattern reliability through statistical variance

#### Output Structure:
```python
{
    "sufficient_data": True,
    "pattern_strength": 0.85,  # Statistical reliability
    "avg_completion_interval_days": 14.5,
    "predicted_next_date": datetime(2025, 7, 2),
    "avg_quality_score": 8.7,
    "avg_efficiency_score": 9.1,
    "most_common_assignee": "John Doe",
    "assignee_distribution": {"John Doe": 5, "Jane Smith": 3},
    "recommendation_confidence": 0.92
}
```

---

### 2. Alert-Triggered Recommendation Analysis

#### `analyze_alert_triggers(location_id: str, alert_severity_threshold: str = "severe") -> Dict[str, Any]`

**Purpose**: Analyzes real-time alert patterns to determine when immediate work orders should be triggered

#### Implementation Strategy:
- **Multi-Tier Alert Analysis**: Critical, severe, and pattern-based triggers
- **Escalation Prediction**: ML-based escalation probability calculation
- **Response Time Optimization**: Dynamic response time recommendations
- **Risk Scoring**: Comprehensive trigger scoring algorithm

#### Key Algorithms:

##### Immediate Trigger Detection:
```python
# Critical alert immediate triggers
critical_alerts = [a for a in active_alerts if a['severity'] in ['critical', 'very_severe']]
severe_alerts = [a for a in active_alerts if a['severity'] == 'severe']

trigger_score = 0.0
if critical_alerts:
    trigger_score += 0.9  # Immediate action required
if len(severe_alerts) >= 2:
    trigger_score += 0.7  # Multiple severe alerts
```

##### Escalation Pattern Analysis:
```python
# Predictive escalation analysis
if escalation_patterns['escalation_probability'] > 0.7:
    trigger_score += 0.6
    trigger_reasons.append("Alert pattern indicates likely escalation to critical")
```

##### Duration-Based Triggers:
```python
# Time-based trigger analysis
long_duration_alerts = [a for a in active_alerts if a['duration_hours'] > 24]
if long_duration_alerts:
    trigger_score += 0.5
    trigger_reasons.append(f"{len(long_duration_alerts)} alerts persisting >24h")
```

#### Analysis Components:
- **Real-Time Alert Monitoring**: Active alert severity and duration tracking
- **Historical Correlation**: Alert-to-work-order success rate analysis
- **Escalation Prediction**: ML-based escalation probability calculation
- **Response Time Estimation**: Dynamic response requirements based on severity

#### Output Structure:
```python
{
    "trigger_score": 0.85,  # 0.0-1.0 trigger urgency
    "should_trigger": True,  # Boolean decision (>= 0.6 threshold)
    "active_alerts_count": 4,
    "critical_alerts": 2,
    "severe_alerts": 1,
    "trigger_reasons": ["2 critical alerts require immediate attention"],
    "escalation_probability": 0.73,
    "recommended_priority": "Urgent",
    "estimated_response_time": "30 minutes"
}
```

---

### 3. Performance Metric Analysis

#### `analyze_metric_performance(location_id: str, work_order_type: str, variance_threshold: float = 0.2) -> Dict[str, Any]`

**Purpose**: Analyzes performance degradation patterns to predict when intervention is needed

#### Implementation Features:
- **Baseline Comparison**: Statistical comparison against historical performance
- **Degradation Detection**: Variance-based performance decline identification
- **Intervention Prediction**: ML-based intervention necessity calculation
- **Improvement Forecasting**: Predicted post-intervention performance gains

#### Key Algorithms:

##### Performance Variance Calculation:
```python
# Statistical performance analysis
for metric_type, recent_values in recent_metrics.items():
    if metric_type in baseline_metrics:
        baseline_avg = baseline_metrics[metric_type]['average']
        recent_avg = sum(recent_values) / len(recent_values)

        # Degradation calculation
        if baseline_avg > 0:
            variance = abs(recent_avg - baseline_avg) / baseline_avg

            performance_indicators[metric_type] = {
                'baseline_average': baseline_avg,
                'recent_average': recent_avg,
                'variance_percentage': variance * 100,
                'degradation_detected': variance > variance_threshold,
                'trend': 'improving' if recent_avg > baseline_avg else 'degrading'
            }
```

##### Degradation Scoring:
```python
# Accumulative degradation scoring
degradation_score = 0.0
for metric_type, indicator in performance_indicators.items():
    if indicator['degradation_detected']:
        degradation_score += indicator['variance_percentage'] / 100.0

# Intervention threshold
intervention_needed = degradation_score > 0.4
```

##### Improvement Prediction:
```python
# Post-intervention improvement forecasting
def _predict_metric_improvement(self, performance_indicators):
    improvements = {}
    for metric, data in performance_indicators.items():
        if data.get('degradation_detected'):
            variance = data['variance_percentage']
            expected_improvement = min(variance * 0.7, 25)  # Cap at 25%
            improvements[metric] = f"{expected_improvement:.1f}% improvement expected"
    return improvements
```

#### Analysis Components:
- **Baseline Establishment**: Historical performance benchmarks
- **Trend Analysis**: Performance trajectory identification
- **Variance Detection**: Statistical significance testing (20% default threshold)
- **Intervention Modeling**: Predictive intervention effectiveness

#### Output Structure:
```python
{
    "degradation_score": 0.45,  # 0.0-1.0 degradation severity
    "intervention_needed": True,  # Boolean decision (>0.4 threshold)
    "performance_indicators": {
        "air_quality": {
            "baseline_average": 75.5,
            "recent_average": 85.2,
            "variance_percentage": 12.8,
            "degradation_detected": False,
            "trend": "degrading"
        }
    },
    "recommendations": ["air_quality showing 12.8% variance from baseline"],
    "confidence": 0.8,
    "predicted_improvement": {"air_quality": "9.0% improvement expected"}
}
```

---

### 4. Employee Availability Analysis

#### `analyze_employee_availability(recommended_time: datetime, preferred_assignees: List[str]) -> Dict[str, Any]`

**Purpose**: Analyzes employee capacity, availability, and optimal assignment scoring

#### Implementation Strategy:
- **Real-Time Availability**: Dynamic workload and conflict analysis
- **Capacity Optimization**: Multi-factor scoring for optimal assignments
- **Skill Matching**: Comprehensive skill-requirement alignment
- **Cost Effectiveness**: Performance-cost ratio optimization

#### Key Algorithms:

##### Availability Calculation:
```python
# Real-time availability analysis
current_workload = self._query_employee_workload(employee.employee_id, recommended_time.date())
scheduled_hours = sum(wo['duration_minutes'] for wo in current_workload) / 60.0
capacity_hours = 8.0  # Standard workday
availability_percentage = max(0, (capacity_hours - scheduled_hours) / capacity_hours * 100)
```

##### Multi-Factor Assignee Scoring:
```python
# Comprehensive assignee scoring algorithm
def _calculate_assignee_score(self, employee, availability_percentage):
    base_score = (employee.efficiency_rating / 10.0) * 0.4
    availability_score = (availability_percentage / 100.0) * 0.3
    cost_score = (1.0 - min(employee.hourly_rate / 50.0, 1.0)) * 0.3
    return base_score + availability_score + cost_score
```

##### Skill Matching Algorithm:
```python
def _calculate_skill_match(self, employee, required_skills):
    if not required_skills:
        return 1.0
    employee_skills = set(skill.skill_name for skill in employee.skills)
    matched_skills = len(set(required_skills).intersection(employee_skills))
    return matched_skills / len(required_skills)
```

#### Analysis Components:
- **Workload Analysis**: Current task load and scheduling conflicts
- **Capacity Planning**: 8-hour standard day capacity management
- **Skill Assessment**: Requirement-capability matching
- **Performance Metrics**: Efficiency rating and cost considerations

#### Output Structure:
```python
{
    "John Doe": {
        "employee_id": "EMP001",
        "scheduled_hours": 6.5,
        "availability_percentage": 18.75,  # Remaining capacity
        "has_conflicts": False,
        "hourly_rate": 25.0,
        "efficiency_rating": 8.5,
        "skill_match_score": 0.85,
        "preferred_zones": ["office", "restroom"],
        "recommendation_score": 0.72  # Multi-factor optimal assignment score
    }
}
```

---

## Business Intelligence Methods

### 5. Performance Insights Generation

#### `generate_performance_insights(recommendations: List[TaskRecommendation]) -> List[Dict[str, Any]]`

**Purpose**: Generates business intelligence insights from recommendation patterns

#### Implementation Features:
- **Pattern Recognition**: Location frequency and source distribution analysis
- **Proactive vs Reactive Analysis**: Strategic maintenance recommendations
- **Confidence Analytics**: Data quality and recommendation reliability assessment
- **Strategic Recommendations**: Actionable business insights

#### Key Algorithms:

##### Location Frequency Analysis:
```python
# High-maintenance location identification
location_frequency = Counter(rec.recommended_location for rec in recommendations)
high_frequency_locations = [loc for loc, count in location_frequency.most_common(3)]

insights.append({
    'type': 'location_frequency',
    'insight': f"Locations requiring frequent attention: {', '.join(high_frequency_locations)}",
    'recommendation': 'Consider preventive maintenance schedules for high-frequency locations'
})
```

##### Reactive Pattern Detection:
```python
# Strategic maintenance analysis
source_distribution = Counter(rec.source.value for rec in recommendations)
alert_driven_percentage = (source_distribution.get('alert_triggered', 0) / len(recommendations)) * 100

if alert_driven_percentage > 60:
    insights.append({
        'type': 'reactive_pattern',
        'insight': f"{alert_driven_percentage:.1f}% of recommendations are reactive",
        'recommendation': 'Increase preventive maintenance to reduce reactive work'
    })
```

#### Insight Categories:
- **Location Intelligence**: High-frequency maintenance locations
- **Strategic Patterns**: Proactive vs reactive maintenance ratios
- **Confidence Analytics**: Data quality and reliability assessment
- **Performance Optimization**: Actionable business recommendations

---

### 6. Business Impact Calculation

#### `calculate_business_impact(recommendation: TaskRecommendation) -> Dict[str, Any]`

**Purpose**: Calculates comprehensive business impact of accepting or rejecting recommendations

#### Implementation Strategy:
- **Source-Based Impact**: Different calculation methods per recommendation source
- **Location Priority**: Facility importance and risk assessment
- **Financial Modeling**: Cost-benefit analysis with savings estimation
- **Risk Quantification**: Business continuity and compliance impact

#### Impact Calculation Methods:

##### Alert-Triggered Impact:
```python
def _calculate_alert_impact(self, recommendation):
    return {
        'type': 'risk_mitigation',
        'estimated_savings': 500.0,  # Emergency response cost avoidance
        'risk_reduction': 'high',
        'service_level_impact': 'critical'
    }
```

##### Pattern-Based Impact:
```python
def _calculate_pattern_impact(self, recommendation):
    return {
        'type': 'efficiency_improvement',
        'estimated_savings': 150.0,  # Preventive maintenance savings
        'risk_reduction': 'medium',
        'service_level_impact': 'positive'
    }
```

##### Location Priority Integration:
```python
# Location-specific impact factors
if location:
    impact['location_priority_multiplier'] = location.cleaning_priority_score / 5.0
    impact['facility_importance'] = 'critical' if location.cleaning_priority_score >= 9.0 else 'standard'
```

#### Impact Categories:
- **Risk Mitigation**: Emergency response cost avoidance
- **Efficiency Improvement**: Preventive maintenance optimization
- **Performance Restoration**: Metric improvement and compliance
- **Facility Importance**: Location-based priority multipliers

---

## Data Integration Methods

### 7. Historical Data Querying

#### Database Integration Functions:

##### `_query_historical_work_orders(location_id, work_order_type, lookback_days)`
**Purpose**: Retrieves historical work order completion data for pattern analysis

```python
query = """
SELECT wo.*, comp.quality_score, comp.efficiency_score,
       comp.actual_start_time, comp.actual_end_time
FROM work_orders wo
LEFT JOIN completion_data comp ON wo.work_order_id = comp.work_order_id
WHERE wo.location LIKE %s
AND wo.work_order_type = %s
AND wo.actual_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
AND wo.status = 'completed'
ORDER BY wo.actual_end_time DESC
"""
```

##### `_query_active_alerts(location_id)`
**Purpose**: Retrieves current active alerts for trigger analysis

```python
query = """
SELECT alert_id, location_id, data_type, severity, value, threshold,
       duration_minutes, timestamp, description
FROM alerts
WHERE location_id = %s
AND status = 'active'
ORDER BY severity DESC, timestamp DESC
"""
```

##### `_query_recent_metrics(location_id, days)`
**Purpose**: Retrieves recent performance metrics for degradation analysis

```python
query = """
SELECT data_type, value, timestamp
FROM metrics
WHERE location_id = %s
AND timestamp >= DATE_SUB(NOW(), INTERVAL %s DAY)
ORDER BY data_type, timestamp
"""
```

### 8. Employee and Location Data Loading

#### Comprehensive Data Loading Functions:

##### `_load_active_employees()`
**Purpose**: Loads complete employee profiles with skills and performance data

```python
query = """
SELECT employee_id, first_name, last_name, hourly_rate,
       efficiency_rating, skill_level, employment_status
FROM employees
WHERE employment_status = 'active'
"""
# Includes skill loading and Employee object creation
```

##### `_load_locations()`
**Purpose**: Loads facility data with priority scoring and coordinates

```python
query = """
SELECT location_id, location_name, zone_type, building, floor,
       cleaning_priority_score, coordinates
FROM locations
WHERE active = 1
"""
# Includes Location object creation with priority scoring
```

##### `_load_task_templates()`
**Purpose**: Loads standardized task templates for consistency

```python
query = """
SELECT template_id, template_name, work_order_type,
       default_duration_minutes, required_skills
FROM task_templates
WHERE active = 1
"""
# Includes TaskTemplate object creation with skill requirements
```

---

## Predictive Analysis Methods

### 9. Alert Escalation Analysis

#### `_analyze_alert_escalation(location_id: str) -> Dict[str, Any]`

**Purpose**: Predicts alert escalation patterns for proactive intervention

#### Implementation:
- **Historical Escalation Patterns**: Analysis of past alert progression
- **Escalation Probability**: ML-based escalation likelihood calculation
- **Prevention Success Rate**: Historical intervention effectiveness
- **Time-to-Escalation**: Predictive escalation timing

#### Mock Implementation:
```python
return {
    'escalation_probability': 0.73,  # 73% chance of escalation
    'avg_escalation_time_hours': 12.5,  # Average time to escalation
    'prevention_success_rate': 0.88  # 88% intervention success rate
}
```

### 10. Baseline Performance Analysis

#### `_query_baseline_metrics(location_id: str, work_order_type: str) -> Dict[str, Any]`

**Purpose**: Establishes performance baselines from completed work orders

#### Implementation Strategy:
- **Historical Performance**: Statistical analysis of completed work orders
- **Metric Baselines**: Average and standard deviation calculations
- **Performance Benchmarks**: Location and task-type specific baselines

#### Mock Implementation:
```python
return {
    'air_quality': {'average': 75.5, 'std_dev': 5.2},
    'temperature': {'average': 72.0, 'std_dev': 2.1},
    'humidity': {'average': 48.0, 'std_dev': 4.8}
}
```

---

## Utility and Helper Methods

### Supporting Analysis Functions:

#### Priority and Response Time Calculation:

##### `_calculate_alert_priority(active_alerts: List[Dict]) -> str`
**Purpose**: Determines work order priority based on alert severity distribution

```python
critical_count = sum(1 for alert in active_alerts if alert['severity'] == 'critical')
severe_count = sum(1 for alert in active_alerts if alert['severity'] in ['severe', 'very_severe'])

if critical_count > 0:
    return "Urgent"
elif severe_count >= 2:
    return "High"
elif severe_count >= 1:
    return "Medium"
else:
    return "Low"
```

##### `_estimate_response_time(active_alerts: List[Dict]) -> str`
**Purpose**: Estimates required response time based on maximum alert severity

```python
response_times = {
    'critical': "30 minutes",
    'very_severe': "2 hours",
    'severe': "4 hours",
    'warning': "24 hours"
}
max_severity = max(alert['severity'] for alert in active_alerts)
return response_times.get(max_severity, "24 hours")
```

#### Employee and Conflict Analysis:

##### `_query_employee_workload(employee_id: str, date) -> List[Dict]`
**Purpose**: Retrieves employee's current workload for availability analysis

##### `_check_time_conflicts(employee_id: str, recommended_time) -> List[Dict]`
**Purpose**: Identifies scheduling conflicts for optimal assignment timing

---

## Mock Data and Testing Support

### Development and Testing Infrastructure:

The RecommendationAnalyzer includes comprehensive mock data support for development and testing:

#### Historical Work Orders:
```python
return [
    {
        'work_order_id': 1001,
        'assignee': 'John Doe',
        'actual_end_time': datetime.now() - timedelta(days=7),
        'quality_score': 8.5,
        'efficiency_score': 9.0
    }
]
```

#### Active Alerts:
```python
return [
    {
        'alert_id': 'A001',
        'location_id': location_id,
        'data_type': 'air_quality',
        'severity': 'severe',
        'duration_hours': 6.5,
        'value': 85,
        'threshold': 70
    }
]
```

#### Recent Metrics:
```python
return {
    'air_quality': [75, 78, 82, 85, 88],
    'temperature': [72, 73, 74, 76, 78],
    'humidity': [45, 47, 50, 52, 55]
}
```

---

## Integration Architecture

### Database Integration Strategy:

The RecommendationAnalyzer follows a flexible database integration pattern:

#### Production Mode:
- **Database Connection**: Full database connectivity for real-time analysis
- **Live Data**: Real-time querying of work orders, alerts, and metrics
- **Performance Optimization**: Indexed queries and efficient data retrieval

#### Development/Testing Mode:
- **Mock Data**: Comprehensive mock data for all analysis functions
- **Consistent Results**: Predictable test data for algorithm validation
- **Development Flexibility**: Easy testing without database dependencies

---

## Performance Characteristics

### Computational Efficiency:
- **Query Optimization**: Indexed database queries with date range filtering
- **Memory Management**: Efficient object loading and caching strategies
- **Algorithm Complexity**: O(n) for most analysis functions, O(nÂ²) for correlation analysis

### Scalability Features:
- **Modular Design**: Independent analysis methods for parallel processing
- **Configurable Parameters**: Adjustable thresholds and timeframes
- **Incremental Analysis**: Support for partial data processing

### Reliability Measures:
- **Confidence Scoring**: ML-style confidence calculation for all recommendations
- **Data Validation**: Minimum data requirements for reliable analysis
- **Fallback Strategies**: Graceful degradation with insufficient data

---

## Strategic Value Proposition

### Business Intelligence:
- **Proactive Maintenance**: Pattern-based preventive recommendations
- **Cost Optimization**: Resource allocation and efficiency improvements
- **Risk Mitigation**: Alert-driven immediate response recommendations
- **Performance Optimization**: Data-driven facility management insights

### Operational Excellence:
- **Predictive Analytics**: ML-based escalation and performance prediction
- **Resource Optimization**: Intelligent employee assignment recommendations
- **Quality Assurance**: Historical performance and improvement tracking
- **Strategic Planning**: Long-term facility maintenance optimization
